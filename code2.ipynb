{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"pwd\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pwd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpwd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pwd'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pwd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delivery_data.isnull().sum()\n",
    "ID                             0\n",
    "Delivery_person_ID             0\n",
    "Delivery_person_Age            0\n",
    "Delivery_person_Ratings        0\n",
    "Restaurant_latitude            0\n",
    "Restaurant_longitude           0\n",
    "Delivery_location_latitude     0\n",
    "Delivery_location_longitude    0\n",
    "Type_of_order                  0\n",
    "Type_of_vehicle                0\n",
    "Time_taken(min)                0\n",
    "Distance_km                    0\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_data = pd.read_csv(\"deliverytime.csv\")\n",
    "delivery_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have calculated the distances between the restaurant and delivery locations which are based on the coordinates , The distance is important column for predicting the delivery time . Over here we have used Haversine formula to find the distances   betwen  2 points on earth surface given with their latitudes and longitudes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, sqrt, atan2\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Differences in coordinates\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# Calculate distance for each row in the DataFrame\n",
    "delivery_data['Distance_km'] = delivery_data.apply(lambda row: haversine(row['Restaurant_latitude'], \n",
    "                                                                         row['Restaurant_longitude'], \n",
    "                                                                         row['Delivery_location_latitude'], \n",
    "                                                                         row['Delivery_location_longitude']), axis=1)\n",
    "\n",
    "# Show the updated DataFrame with the new column\n",
    "delivery_data[['Restaurant_latitude', 'Restaurant_longitude', 'Delivery_location_latitude', 'Delivery_location_longitude', 'Distance_km']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the visualization configuration\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical features\n",
    "numerical_summary = delivery_data[['Delivery_person_Age', 'Delivery_person_Ratings', 'Distance_km', 'Time_taken(min)']].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distributions of numerical features\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\n",
    "sns.histplot(delivery_data['Delivery_person_Age'], bins=20, kde=True, ax=axes[0, 0]).set_title('Distribution of Delivery Person Age')\n",
    "sns.histplot(delivery_data['Delivery_person_Ratings'], bins=20, kde=True, ax=axes[0, 1]).set_title('Distribution of Delivery Person Ratings')\n",
    "sns.histplot(delivery_data['Distance_km'], bins=20, kde=True, ax=axes[1, 0]).set_title('Distribution of Distance (km)')\n",
    "sns.histplot(delivery_data['Time_taken(min)'], bins=20, kde=True, ax=axes[1, 1]).set_title('Distribution of Time Taken (min)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delivery_person_Age: The age ranges from 15 to 50 years with a mean of approximately 29.5 years. The distribution is fairly uniform with a slight skew towards younger ages.\n",
    "\n",
    "Delivery_person_Ratings: Ratings are mostly high, clustered around 4.6 to 4.8, with an overall range from 1.0 to 6.0. Note that there are ratings above 5, which might indicate data entry errors or a different rating scale.\n",
    "\n",
    "Distance_km: Most distances are relatively short, though there are extreme values (up to 19,692 km, which is unrealistic and likely indicates data issues). The median distance is around 9.26 km, suggesting most deliveries are local.\n",
    "\n",
    "Time_taken(min): Delivery times range from 10 to 54 minutes, with an average delivery time of about 26 minutes. The distribution is fairly normal but slightly skewed right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Issues and Outliers:\n",
    "\n",
    "There are apparent outliers in the Distance_km that could be due to data entry errors or incorrect geolocation data. These need to be addressed as they can significantly affect model accuracy.\n",
    "The Delivery_person_Ratings column has values above the typical maximum of 5, suggesting either a different scale or erroneous entries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning issues  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting Ratings: Since ratings should logically be on a scale from 1 to 5, any ratings above 5 are likely erroneous and need to be addressed. We can set a strategy to either cap them at 5 or investigate these entries further to determine the correct action.\n",
    "Handling Outliers in Distance: Extremely large values in the Distance_km seem unrealistic, especially for food deliveries. We'll set a reasonable maximum threshold for distances and investigate entries exceeding this threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping ratings at 5.0\n",
    "delivery_data['Delivery_person_Ratings'] = delivery_data['Delivery_person_Ratings'].apply(lambda x: min(x, 5.0))\n",
    "\n",
    "# Analyze the distribution of Distance_km to decide on a threshold for capping\n",
    "distance_percentiles = delivery_data['Distance_km'].quantile([0.90, 0.95, 0.99])\n",
    "\n",
    "distance_percentiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 99th percentile for the distance is approximately 20.97 km. Given this analysis, a reasonable threshold to consider capping the distance might be around 21 km, which would help manage the extreme outliers without affecting the majority of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping distance at 21 km\n",
    "delivery_data['Distance_km'] = delivery_data['Distance_km'].apply(lambda x: min(x, 21.0))\n",
    "\n",
    "# Plot the updated distribution of Distance_km\n",
    "sns.histplot(delivery_data['Distance_km'], bins=20, kde=True).set_title('Updated Distribution of Distance (km)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated distribution of Distance_km now reflects a more realistic range, with all distances capped at 21 km, addressing the issue of extreme outliers effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# examining the categorical features Type_of_order and Type_of_vehicle in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the frequency of each category in Type_of_order and Type_of_vehicle\n",
    "order_type_counts = delivery_data['Type_of_order'].value_counts()\n",
    "vehicle_type_counts = delivery_data['Type_of_vehicle'].value_counts()\n",
    "\n",
    "# Plotting the distributions\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
    "sns.barplot(x=order_type_counts.index, y=order_type_counts.values, ax=axes[0]).set_title('Distribution of Type of Order')\n",
    "sns.barplot(x=vehicle_type_counts.index, y=vehicle_type_counts.values, ax=axes[1]).set_title('Distribution of Type of Vehicle')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "order_type_counts, vehicle_type_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Feature Analysis:\n",
    "Type of Order:\n",
    "\n",
    "The orders are quite evenly distributed across four categories: Snack, Meal, Drinks, and Buffet.\n",
    "Type of Vehicle:\n",
    "\n",
    "The majority of deliveries are made using motorcycles, followed by scooters, and then electric scooters. Bicycles are very rarely used.\n",
    "These distributions suggest that both order type and vehicle type are well-represented in the dataset, which allows for a comprehensive analysis of their impacts on delivery times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode the categorical variables Type_of_order and Type_of_vehicle using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding the categorical variables\n",
    "categorical_encoded = pd.get_dummies(delivery_data[['Type_of_order', 'Type_of_vehicle']], drop_first=True)\n",
    "\n",
    "# Joining the encoded columns back to the original DataFrame\n",
    "delivery_data_encoded = delivery_data.join(categorical_encoded)\n",
    "\n",
    "# Display the updated DataFrame with encoded categorical variables\n",
    "delivery_data_encoded.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude non-numeric columns and recompute the correlation matrix\n",
    "numeric_data = delivery_data_encoded.select_dtypes(include=['float64', 'int64', 'uint8'])\n",
    "correlation_matrix_numeric = numeric_data.corr()\n",
    "\n",
    "# Visualizing the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix_numeric, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "plt.title('Correlation Matrix for Numeric Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features selection\n",
    "features = numeric_data.drop(columns=['Time_taken(min)'])  # Drop non-feature columns\n",
    "target = numeric_data['Time_taken(min)']  # Target variable\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the train and test sets\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been split into training and testing sets, with 36,474 observations for training and 9,119 for testing, across 13 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate MAE and RMSE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "mae, rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error (MAE): 6.26 minutes\n",
    "Root Mean Squared Error (RMSE): 8.03 minutes\n",
    "These metrics indicate that, on average, the model's predictions are about 6.26 minutes off from the actual delivery times, and the RMSE gives a sense of the standard deviation of the prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "The results suggest that the model has a reasonable level of accuracy but might still benefit from further tuning or possibly using additional features. The errors are not too high, which means the model can provide a decent baseline for predicting delivery times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Tuning: Adjusting model parameters or trying more sophisticated algorithms to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Reducing the training data size for quicker grid search (using 10% of the training data)\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
    "\n",
    "# Simplified parameter grid\n",
    "simplified_param_grid = {\n",
    "    'n_estimators': [100, 150],  # Fewer options for number of trees\n",
    "    'max_depth': [None, 20],  # Fewer depth options\n",
    "    'min_samples_split': [2, 5]  # Fewer splitting options\n",
    "}\n",
    "\n",
    "# Create a simplified GridSearchCV object\n",
    "simplified_grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),\n",
    "                                      param_grid=simplified_param_grid, \n",
    "                                      cv=3,  # Keeping cross-validation\n",
    "                                      scoring='neg_mean_squared_error',  # Using MSE for scoring\n",
    "                                      verbose=1)\n",
    "\n",
    "# Perform the simplified grid search\n",
    "simplified_grid_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Best parameters and best score from the simplified grid search\n",
    "simplified_best_params = simplified_grid_search.best_params_\n",
    "simplified_best_score = sqrt(-simplified_grid_search.best_score_)  # Convert MSE to RMSE\n",
    "\n",
    "simplified_best_params, simplified_best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:\n",
    "n_estimators: 100 (Number of trees)\n",
    "max_depth: None (No maximum depth)\n",
    "min_samples_split: 5 (Minimum number of samples required to split a node)\n",
    "Best RMSE: 7.98 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest Regressor with the best parameters from the grid search\n",
    "optimized_rf_model = RandomForestRegressor(n_estimators=simplified_best_params['n_estimators'],\n",
    "                                           max_depth=simplified_best_params['max_depth'],\n",
    "                                           min_samples_split=simplified_best_params['min_samples_split'],\n",
    "                                           random_state=42)\n",
    "\n",
    "# Train the optimized model on the full training set\n",
    "optimized_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "optimized_y_pred = optimized_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate MAE and RMSE for the optimized model\n",
    "optimized_mae = mean_absolute_error(y_test, optimized_y_pred)\n",
    "optimized_rmse = sqrt(mean_squared_error(y_test, optimized_y_pred))\n",
    "\n",
    "optimized_mae, optimized_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the model for quicker cross-validation by reducing the number of estimators\n",
    "quick_rf_model = RandomForestRegressor(n_estimators=50, max_depth=simplified_best_params['max_depth'],\n",
    "                                       min_samples_split=simplified_best_params['min_samples_split'], random_state=42)\n",
    "\n",
    "# Performing 5-fold cross-validation with the adjusted model\n",
    "quick_cv_scores = cross_val_score(quick_rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate RMSE for each fold\n",
    "quick_cv_rmse_scores = np.sqrt(-quick_cv_scores)\n",
    "\n",
    "# Summary of cross-validation RMSE scores\n",
    "quick_cv_rmse_mean = np.mean(quick_cv_rmse_scores)\n",
    "quick_cv_rmse_std = np.std(quick_cv_rmse_scores)\n",
    "\n",
    "quick_cv_rmse_scores, quick_cv_rmse_mean, quick_cv_rmse_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances from the model\n",
    "feature_importances = optimized_rf_model.feature_importances_\n",
    "\n",
    "# Creating a DataFrame to view the features and their importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': features.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Visualizing the feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importances in Random Forest Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "feature_importance_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delivery_person_Ratings: This feature has the highest importance, indicating that the ratings of a delivery person are a strong predictor of delivery times.\n",
    "Distance_km: As expected, the distance between the restaurant and the delivery location is also a significant predictor.\n",
    "Delivery_person_Age: The age of the delivery person shows moderate importance, suggesting it has some but not a dominant influence on delivery times.\n",
    "Geographical Coordinates: The longitude and latitude for both the delivery location and the restaurant have varying degrees of importance, likely reflecting the geographical factors that affect delivery times, such as urban density or traffic patterns.\n",
    "These results can guide business decisions and model refinement. For instance, the high importance of delivery person ratings could suggest investing in training or incentives for delivery personnel to maintain high service quality, which apparently correlates with faster delivery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already loaded your dataset and split it into training and testing sets\n",
    "# and your model 'optimized_rf_model' is already trained\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = optimized_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate residuals (differences between actual and predicted values)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plotting the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='red', linestyle='--')  # A line at zero which ideally should have all points\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
